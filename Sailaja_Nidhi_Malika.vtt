WEBVTT

a90a9c17-c245-4aa4-b94a-d07c0fa426ab/39-0
00:00:18.020 --> 00:00:22.251
Good morning everyone. This is
Nidhi, this is Sheila. This is

a90a9c17-c245-4aa4-b94a-d07c0fa426ab/39-1
00:00:22.251 --> 00:00:26.415
Divya. We are here to present
our data project water quality

a90a9c17-c245-4aa4-b94a-d07c0fa426ab/39-2
00:00:26.415 --> 00:00:28.940
analysis. The aim of our project
is.

a90a9c17-c245-4aa4-b94a-d07c0fa426ab/82-0
00:00:30.060 --> 00:00:33.861
Classifying of water as safer,
unsafe based on the minerals

a90a9c17-c245-4aa4-b94a-d07c0fa426ab/82-1
00:00:33.861 --> 00:00:37.600
present, and so we are given
compounds aquarium modularly,

a90a9c17-c245-4aa4-b94a-d07c0fa426ab/82-2
00:00:37.600 --> 00:00:41.465
our Senate etcetera and all our
float values. So given those

a90a9c17-c245-4aa4-b94a-d07c0fa426ab/82-3
00:00:41.465 --> 00:00:45.393
values we have to put it in the
water is safe or not. That is

a90a9c17-c245-4aa4-b94a-d07c0fa426ab/82-4
00:00:45.393 --> 00:00:46.280
safe is 1 and.

a90a9c17-c245-4aa4-b94a-d07c0fa426ab/84-0
00:00:47.350 --> 00:00:47.810
It is.

a90a9c17-c245-4aa4-b94a-d07c0fa426ab/88-0
00:00:48.650 --> 00:00:49.530
Until the zero.

a90a9c17-c245-4aa4-b94a-d07c0fa426ab/128-0
00:00:51.310 --> 00:00:55.563
And I don't I alternate aim of
our project is, uh, how we

a90a9c17-c245-4aa4-b94a-d07c0fa426ab/128-1
00:00:55.563 --> 00:00:59.817
balance how we deal with
imbalance data. Since imbalanced

a90a9c17-c245-4aa4-b94a-d07c0fa426ab/128-2
00:00:59.817 --> 00:01:04.437
data is a major problem with all
the algorithms we applied PCA

a90a9c17-c245-4aa4-b94a-d07c0fa426ab/128-3
00:01:04.437 --> 00:01:06.050
and smart in order to.

a90a9c17-c245-4aa4-b94a-d07c0fa426ab/139-0
00:01:06.720 --> 00:01:09.878
Generate a method uh, which
nullifies the effect of

a90a9c17-c245-4aa4-b94a-d07c0fa426ab/139-1
00:01:09.878 --> 00:01:10.850
imbalanced data.

a90a9c17-c245-4aa4-b94a-d07c0fa426ab/203-0
00:01:11.760 --> 00:01:15.720
Uh so. And we tried different
models like logistic regression,

a90a9c17-c245-4aa4-b94a-d07c0fa426ab/203-1
00:01:15.720 --> 00:01:19.555
SVM, decision trees, random
forest and our aim is to see how

a90a9c17-c245-4aa4-b94a-d07c0fa426ab/203-2
00:01:19.555 --> 00:01:23.641
we can, how we can deal with the
class imbalance given the data.

a90a9c17-c245-4aa4-b94a-d07c0fa426ab/203-3
00:01:23.641 --> 00:01:27.727
So we first performed EDA where
we had to clean the data set and

a90a9c17-c245-4aa4-b94a-d07c0fa426ab/203-4
00:01:27.727 --> 00:01:31.624
we thoroughly analyzed how the
data is distributed and that's

a90a9c17-c245-4aa4-b94a-d07c0fa426ab/203-5
00:01:31.624 --> 00:01:35.270
where we were able to identify
there's a class imbalance.

a90a9c17-c245-4aa4-b94a-d07c0fa426ab/205-0
00:01:36.300 --> 00:01:36.760
Umm.

a90a9c17-c245-4aa4-b94a-d07c0fa426ab/251-0
00:01:38.120 --> 00:01:42.080
As the first uh, we see logistic
regression. Uh, easily logistic

a90a9c17-c245-4aa4-b94a-d07c0fa426ab/251-1
00:01:42.080 --> 00:01:46.101
regression and we perform uh, we
scale the data and then we train

a90a9c17-c245-4aa4-b94a-d07c0fa426ab/251-2
00:01:46.101 --> 00:01:50.123
the logistic regression with the
scale data with ECA and with PCA

a90a9c17-c245-4aa4-b94a-d07c0fa426ab/251-3
00:01:50.123 --> 00:01:53.230
plus mode. So what we observed
was that the model.

a90a9c17-c245-4aa4-b94a-d07c0fa426ab/327-0
00:01:53.900 --> 00:01:58.370
Did not have a good accuracy
with only PCA, but when it hired

a90a9c17-c245-4aa4-b94a-d07c0fa426ab/327-1
00:01:58.370 --> 00:02:02.480
with only PCA the accuracy
dropped and the recall of the

a90a9c17-c245-4aa4-b94a-d07c0fa426ab/327-2
00:02:02.480 --> 00:02:06.806
minority class also was like a
very less like 34%. But then

a90a9c17-c245-4aa4-b94a-d07c0fa426ab/327-3
00:02:06.806 --> 00:02:11.420
using PCA plus mode boosted up
the recall of the minority class

a90a9c17-c245-4aa4-b94a-d07c0fa426ab/327-4
00:02:11.420 --> 00:02:15.530
and the F1 score same as the
case with SVM. It performed

a90a9c17-c245-4aa4-b94a-d07c0fa426ab/327-5
00:02:15.530 --> 00:02:19.928
better with smart and PCA. This
decision tree also performed

a90a9c17-c245-4aa4-b94a-d07c0fa426ab/327-6
00:02:19.928 --> 00:02:24.110
well. It's modern PCA with
respect to minority class and.

a90a9c17-c245-4aa4-b94a-d07c0fa426ab/415-0
00:02:24.260 --> 00:02:28.092
We've similarly with and. The
reason we we think PC and plus

a90a9c17-c245-4aa4-b94a-d07c0fa426ab/415-1
00:02:28.092 --> 00:02:31.987
mode performs better is because
because of number of features

a90a9c17-c245-4aa4-b94a-d07c0fa426ab/415-2
00:02:31.987 --> 00:02:35.882
and a class imbalance, when we
apply PCA on this PPA does not

a90a9c17-c245-4aa4-b94a-d07c0fa426ab/415-3
00:02:35.882 --> 00:02:39.965
do that well when treating with
class and balance. So what we do

a90a9c17-c245-4aa4-b94a-d07c0fa426ab/415-4
00:02:39.965 --> 00:02:43.483
is we perform PCA and then we
use more to synthetically

a90a9c17-c245-4aa4-b94a-d07c0fa426ab/415-5
00:02:43.483 --> 00:02:47.001
oversample the minority class.
This way the accuracy is

a90a9c17-c245-4aa4-b94a-d07c0fa426ab/415-6
00:02:47.001 --> 00:02:50.581
improved do improve random
forest. Is one such algorithm

a90a9c17-c245-4aa4-b94a-d07c0fa426ab/415-7
00:02:50.581 --> 00:02:54.602
that does not get affected with
class imbalance and it gives an

a90a9c17-c245-4aa4-b94a-d07c0fa426ab/415-8
00:02:54.602 --> 00:02:55.670
accuracy of .93%.

a90a9c17-c245-4aa4-b94a-d07c0fa426ab/502-0
00:02:55.740 --> 00:02:59.681
And there's good of .85% we aim
to build an ensemble model which

a90a9c17-c245-4aa4-b94a-d07c0fa426ab/502-1
00:02:59.681 --> 00:03:02.955
gives similar or similar
accuracy and if one hence we

a90a9c17-c245-4aa4-b94a-d07c0fa426ab/502-2
00:03:02.955 --> 00:03:06.715
build an ensample model with
with linear logistic regression,

a90a9c17-c245-4aa4-b94a-d07c0fa426ab/502-3
00:03:06.715 --> 00:03:10.474
decision trees and SVM and we
train it on this more data set.

a90a9c17-c245-4aa4-b94a-d07c0fa426ab/502-4
00:03:10.474 --> 00:03:14.173
More plus PCA data and we saw
that it was performing as good

a90a9c17-c245-4aa4-b94a-d07c0fa426ab/502-5
00:03:14.173 --> 00:03:17.872
as the random forest. So that
what we would like to say here

a90a9c17-c245-4aa4-b94a-d07c0fa426ab/502-6
00:03:17.872 --> 00:03:21.570
is that using PCA plus mode on
week learners for an ensemble

a90a9c17-c245-4aa4-b94a-d07c0fa426ab/502-7
00:03:21.570 --> 00:03:25.208
model does help in increasing
the accuracy and it does help

a90a9c17-c245-4aa4-b94a-d07c0fa426ab/502-8
00:03:25.208 --> 00:03:26.300
you with in class.

a90a9c17-c245-4aa4-b94a-d07c0fa426ab/504-0
00:03:26.390 --> 00:03:26.950
But.

a90a9c17-c245-4aa4-b94a-d07c0fa426ab/507-0
00:03:31.970 --> 00:03:32.270
Thank you.

a90a9c17-c245-4aa4-b94a-d07c0fa426ab/569-0
00:04:27.220 --> 00:04:31.459
OK, so our data set contains
around seven 7999 rows and 21

a90a9c17-c245-4aa4-b94a-d07c0fa426ab/569-1
00:04:31.459 --> 00:04:35.986
columns where one column is is
the is the target loss. So when

a90a9c17-c245-4aa4-b94a-d07c0fa426ab/569-2
00:04:35.986 --> 00:04:40.512
when we were running performing
a RDM we observed that that is

a90a9c17-c245-4aa4-b94a-d07c0fa426ab/569-3
00:04:40.512 --> 00:04:44.895
we were able to identify if
there was a class in balance and

a90a9c17-c245-4aa4-b94a-d07c0fa426ab/569-4
00:04:44.895 --> 00:04:47.410
we were able to identify how
many.

a90a9c17-c245-4aa4-b94a-d07c0fa426ab/611-0
00:04:48.660 --> 00:04:51.379
Ohh, the minerals had a
threshold above a particular

a90a9c17-c245-4aa4-b94a-d07c0fa426ab/611-1
00:04:51.379 --> 00:04:54.405
value which would identify them
as safe or not which we're

a90a9c17-c245-4aa4-b94a-d07c0fa426ab/611-2
00:04:54.405 --> 00:04:57.329
identifying. If they were
dangerous or not dangerous. So

a90a9c17-c245-4aa4-b94a-d07c0fa426ab/611-3
00:04:57.329 --> 00:05:00.612
we found that fluoride, selenium
and uranium were in the entire

a90a9c17-c245-4aa4-b94a-d07c0fa426ab/611-4
00:05:00.612 --> 00:05:02.920
data set, did not cross the
threshold value.

a90a9c17-c245-4aa4-b94a-d07c0fa426ab/642-0
00:05:04.620 --> 00:05:08.358
Hope you also observe that some
of there was some of the nine

a90a9c17-c245-4aa4-b94a-d07c0fa426ab/642-1
00:05:08.358 --> 00:05:12.218
values present. Uh. So assuming
that the concentration of those

a90a9c17-c245-4aa4-b94a-d07c0fa426ab/642-2
00:05:12.218 --> 00:05:16.077
minerals were very small to be,
uh, monitored, we attended that

a90a9c17-c245-4aa4-b94a-d07c0fa426ab/642-3
00:05:16.077 --> 00:05:16.680
with zero.

a90a9c17-c245-4aa4-b94a-d07c0fa426ab/651-0
00:05:22.660 --> 00:05:24.610
Killers are cold, uh, indrani.

a90a9c17-c245-4aa4-b94a-d07c0fa426ab/740-0
00:06:00.640 --> 00:06:04.597
So the models they train on as
far as the logistic regression

a90a9c17-c245-4aa4-b94a-d07c0fa426ab/740-1
00:06:04.597 --> 00:06:08.681
and then we perform a try to see
the how the model perform with

a90a9c17-c245-4aa4-b94a-d07c0fa426ab/740-2
00:06:08.681 --> 00:06:12.702
scaling PC and with PC address
mode then we did for SVM and as

a90a9c17-c245-4aa4-b94a-d07c0fa426ab/740-3
00:06:12.702 --> 00:06:16.914
which is a classical like a very
classic classification model and

a90a9c17-c245-4aa4-b94a-d07c0fa426ab/740-4
00:06:16.914 --> 00:06:20.871
we then moved on to naive bias
which because naive vice has a

a90a9c17-c245-4aa4-b94a-d07c0fa426ab/740-5
00:06:20.871 --> 00:06:24.317
property that it teaches it. If
all your features are

a90a9c17-c245-4aa4-b94a-d07c0fa426ab/740-6
00:06:24.317 --> 00:06:28.337
independent of each other and it
gives equal importance to all

a90a9c17-c245-4aa4-b94a-d07c0fa426ab/740-7
00:06:28.337 --> 00:06:31.720
the features and then we move
them to random forest.

a90a9c17-c245-4aa4-b94a-d07c0fa426ab/754-0
00:06:31.820 --> 00:06:34.833
Decision trees and from a
decision tree is CMLR is we what

a90a9c17-c245-4aa4-b94a-d07c0fa426ab/754-1
00:06:34.833 --> 00:06:36.570
we build the ensemble model
with.

a90a9c17-c245-4aa4-b94a-d07c0fa426ab/781-0
00:06:38.070 --> 00:06:42.603
And then we use that KN and XG
boost. XG Boost is robust to

a90a9c17-c245-4aa4-b94a-d07c0fa426ab/781-1
00:06:42.603 --> 00:06:46.910
class imbalance, so it also
performed pretty well on the

a90a9c17-c245-4aa4-b94a-d07c0fa426ab/781-2
00:06:46.910 --> 00:06:47.590
data set.

a90a9c17-c245-4aa4-b94a-d07c0fa426ab/786-0
00:06:58.690 --> 00:06:59.580
And thank you.